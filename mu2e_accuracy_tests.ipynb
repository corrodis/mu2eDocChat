{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f177e6f1-4999-47bf-9337-971a377c30a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import anthropic\n",
    "import mu2e\n",
    "import json\n",
    "import os\n",
    "\n",
    "client = anthropic.Anthropic(api_key=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d7615a3-022e-49cc-9bfd-237830870502",
   "metadata": {},
   "outputs": [],
   "source": [
    "#generating first system prompt\n",
    "def assignPrompt1(q):\n",
    "\n",
    "        website_content_string = \"\"\n",
    "        for doc in mu2e.findAndGet(q):\n",
    "            website_content_string += \"<page url=\\\"\"+doc['url']+\"\\\" title=\\\"\"+doc['heading']+\"\\\">\\n\"+doc['text']+\"</page>\\n\"\n",
    "\n",
    "        SYSTEM_PROMPT = f\"\"\"You are a mu2e DocBot, a helpful assistant that is an expert at helping users with the mu2e related questions.\n",
    "\n",
    "        Here is some related Mu2e documentation\n",
    "\t<documentation>\n",
    "\t{website_content_string}\n",
    "\t</documentation>\n",
    "\n",
    "        When a user asks a question, peform the following tasks:\n",
    "        1. Find sections in the documentation that are the most relevant to answering the question. You may need to use multiple pages.\n",
    "        2. Keep track of the url of the pages.\n",
    "        3. Based on the document, answer the question. Directly quote the documentation when possible, including examples.\n",
    "        4.  When answering the question provide references to the corresponding urls in square brackets from step 2.\n",
    "        Format the answer as plain text, formatted as Markdown[1]\n",
    "\t\"\"\"\n",
    "        return SYSTEM_PROMPT\n",
    "\n",
    "\n",
    "#second\n",
    "def assignPrompt2(q):\n",
    "\n",
    "        website_content_string = \"\"\n",
    "        for doc in mu2e.findAndGet(q):\n",
    "            website_content_string += \"<page url=\\\"\"+doc['url']+\"\\\" title=\\\"\"+doc['heading']+\"\\\">\\n\"+doc['text']+\"</page>\\n\"\n",
    "\n",
    "        SYSTEM_PROMPT = f\"\"\"You are a mu2e DocBot, a helpful assistant that is an expert at helping users with the mu2e related questions.\n",
    "\n",
    "        Here is some related Mu2e documentation\n",
    "\t<documentation>\n",
    "\t{website_content_string}\n",
    "\t</documentation>\n",
    "\n",
    "    Give a concise, accurate answer using the information provided.\n",
    "    \n",
    "\t\"\"\"\n",
    "        return SYSTEM_PROMPT\n",
    "\n",
    "#third\n",
    "def assignPrompt3(q):\n",
    "\n",
    "        website_content_string = \"\"\n",
    "        for doc in mu2e.findAndGet(q):\n",
    "            website_content_string += \"<page url=\\\"\"+doc['url']+\"\\\" title=\\\"\"+doc['heading']+\"\\\">\\n\"+doc['text']+\"</page>\\n\"\n",
    "\n",
    "        SYSTEM_PROMPT = f\"\"\"You are a mu2e DocBot, a helpful assistant that is an expert at helping users with the mu2e related questions.\n",
    "\n",
    "        Here is some related Mu2e documentation\n",
    "\t<documentation>\n",
    "\t{website_content_string}\n",
    "\t</documentation>\n",
    "    \n",
    "\t\"\"\"\n",
    "        return SYSTEM_PROMPT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f220b8c0-22dc-4f51-907d-e06af3a25aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_claude(q, system_prompt):\n",
    "    message = client.messages.create(\n",
    "    model = \"claude-3-haiku-20240307\",\n",
    "    system = system_prompt,\n",
    "    max_tokens = 512,\n",
    "    messages = [\n",
    "        {\"role\":\"user\", \"content\": q }\n",
    "        ]\n",
    "    )\n",
    "    return message.content[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8bf6a01-bfe3-4c94-b37f-8053711c6246",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example: q = \"What is the primary purpose of the Mu2e experiment?\"\n",
    "response1 = ask_claude(q, assignPrompt1(q))\n",
    "response2 = ask_claude(q, assignPrompt2(q))\n",
    "response3 = ask_claude(q, assignPrompt3(q))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc075b8-9d23-4f68-9043-b4368c2a03ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "website_content_string = \"\"\n",
    "for doc in mu2e.findAndGet(q):\n",
    "    website_content_string += \"<page url=\\\"\"+doc['url']+\"\\\" title=\\\"\"+doc['heading']+\"\\\">\\n\"+doc['text']+\"</page>\\n\"\n",
    "\n",
    "message = client.messages.create(\n",
    "    model = \"claude-3-haiku-20240307\",\n",
    "    max_tokens = 300,\n",
    "    messages = [\n",
    "        {\"role\":\"user\", \"content\": f'''\n",
    "        \n",
    "        I asked you: {q}\n",
    "        \n",
    "        I got these three responses:\n",
    "\n",
    "        first response = {response1}\n",
    "        second response = {response2}\n",
    "        third response = {response3}\n",
    "\n",
    "        The data these responses are based on can be found:\n",
    "        <documentation>\n",
    "    \t{website_content_string}\n",
    "    \t</documentation>\n",
    "\n",
    "        Which response is the best? Rank the three responses.'''\n",
    "        \n",
    "        \n",
    "        }\n",
    "    ]\n",
    ")\n",
    "print(message.content[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8edfcdbb-9f2d-4736-a2b3-765297fc080b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#finding cosine similarity\n",
    "from transformers import AutoTokenizer\n",
    "from angle_emb import AnglE\n",
    "angle = AnglE.from_pretrained('WhereIsAI/UAE-Large-V1', pooling_strategy='cls').cuda()\n",
    "\n",
    "embeddingResponse1 = angle.encode([response1])[0]\n",
    "embeddingResponse2 = angle.encode([response2])[0]\n",
    "embeddingResponse3 = angle.encode([response3])[0]\n",
    "\n",
    "embeddingResponse1 = embeddingResponse1.reshape(1, -1)\n",
    "embeddingResponse2 = embeddingResponse2.reshape(1, -1)\n",
    "embeddingResponse3 = embeddingResponse3.reshape(1, -1)\n",
    "\n",
    "embeddingQuestion = angle.encode([q])[0]\n",
    "embeddingQuestion = embeddingQuestion.reshape(1, -1)\n",
    "\n",
    "embeddingPrompt1 = angle.encode([assignPrompt(q)])[0]\n",
    "embeddingPrompt2 = angle.encode([assignPrompt2(q)])[0]\n",
    "embeddingPrompt3 = angle.encode([assignPrompt3(q)])[0]\n",
    "\n",
    "embeddingPrompt1 = embeddingPrompt1.reshape(1, -1)\n",
    "embeddingPrompt2 = embeddingPrompt2.reshape(1, -1)\n",
    "embeddingPrompt3 = embeddingPrompt3.reshape(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b867e3d-c9c9-4717-88f7-e5efc6499c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "cosineSimP1 = cosine_similarity(embeddingQuestion, embeddingPrompt1)[0][0]\n",
    "cosineSimP2 = cosine_similarity(embeddingQuestion, embeddingPrompt2)[0][0]\n",
    "cosineSimP3 = cosine_similarity(embeddingQuestion, embeddingPrompt3)[0][0]\n",
    "cosineSimR1 = cosine_similarity(embeddingQuestion, embeddingResponse1)[0][0]\n",
    "cosineSimR2 = cosine_similarity(embeddingQuestion, embeddingResponse2)[0][0]\n",
    "cosineSimR3 = cosine_similarity(embeddingQuestion, embeddingResponse3)[0][0]\n",
    "\n",
    "print(cosineSimP1)\n",
    "print(cosineSimP2)\n",
    "print(cosineSimP3)\n",
    "print(cosineSimR1)\n",
    "print(cosineSimR2)\n",
    "print(cosineSimR3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b29d6fd-b817-4814-a47b-61f9b5e68ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#finding rouge score\n",
    "from rouge_score import rouge_scorer\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "\n",
    "candidate_summaries = [response1, response2, response3]\n",
    "reference = mu2e.findAndGet(q)\n",
    "reference_summary = \"\"\n",
    "for e in reference:\n",
    "    reference_summary += str(e)\n",
    "\n",
    "scores = {key: [] for key in ['rouge1', 'rouge2', 'rougeL']}\n",
    "for can in candidate_summaries:\n",
    "    temp_scores = scorer.score(reference_summary, can)\n",
    "    for key in temp_scores:\n",
    "        scores[key].append(temp_scores[key])\n",
    "\n",
    "for key in scores:\n",
    "    print(f'{key}:\\n{scores[key]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc3e187-6414-46b0-b209-d005e3b7b45d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#finding BERTScore\n",
    "\n",
    "# Step 1: Import the required libraries\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Step 2: Load the pre-trained BERT model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Step 3: Define the two texts to compare\n",
    "\n",
    "def find_BERTScore(reference, response):\n",
    "    text1 = text1\n",
    "    text2 = text2\n",
    "#text1 = reference_summary\n",
    "\n",
    "#responses = [response1, response2, response3]\n",
    "#text2 = response you are comparing\n",
    "#text2 = response2\n",
    "\n",
    "    # Step 4: Prepare the texts for BERT\n",
    "    inputs1 = tokenizer(text1, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    inputs2 = tokenizer(text2, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "    # Step 5: Feed the texts to the BERT model\n",
    "    outputs1 = model(**inputs1)\n",
    "    outputs2 = model(**inputs2)\n",
    "\n",
    "    # Step 6: Obtain the representation vectors\n",
    "    embeddings1 = outputs1.last_hidden_state.mean(dim=1).detach().numpy()\n",
    "    embeddings2 = outputs2.last_hidden_state.mean(dim=1).detach().numpy()\n",
    "\n",
    "    # Step 7: Calculate cosine similarity\n",
    "    similarity = np.dot(embeddings1, embeddings2.T) / (np.linalg.norm(embeddings1) * np.linalg.norm(embeddings2))\n",
    "\n",
    "    # Step 8: Print the result\n",
    "    print(\"Similarity between the texts: {:.4f}\".format(similarity[0][0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11bd6f14-4b3e-4dea-9512-730759b06adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "responses = [response1, response2, response3]\n",
    "\n",
    "for response in responses:\n",
    "    print(\"Calculating BERTScore between reference text and response.\")\n",
    "    find_BERTScore(reference_summary, response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f97e0b-ff5e-414e-b8f5-b208edca4279",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "claude-env",
   "language": "python",
   "name": "claude-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
